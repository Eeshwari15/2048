{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summariation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i2f3a6JcyMvy",
        "3YNLdvbazJ-O",
        "JlH_L5sFzxu2",
        "wmHNVyZBlvlP",
        "qHbhj7jnq_03",
        "thD7xCYA2q_w",
        "9hCzI2C93fVL",
        "DA4dgXw63kPd",
        "LOywr6nv37-C",
        "REbXC96Glz94",
        "RGOrHjeUyjJ9",
        "UhnykjaiFlR3"
      ],
      "mount_file_id": "1uV4-iRg3_yfO3tIqQICTJ4YhwynQnBmX",
      "authorship_tag": "ABX9TyMe0Q+eqovjm8hAuoJvrVdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eeshwari15/2048/blob/master/Summariation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2f3a6JcyMvy",
        "colab_type": "text"
      },
      "source": [
        "# Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-dxn7vDybK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install googletrans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV5tJSTkyPiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16dcbe7f-9f3e-48bb-a984-1c57c0456516"
      },
      "source": [
        "import googletrans\n",
        "\n",
        "print(len(googletrans.LANGUAGES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNrQepNyzot",
        "colab_type": "text"
      },
      "source": [
        "## Detect the type of language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bnMAxAyrgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "96d9911e-68ee-4718-91e5-f140fb51da87"
      },
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "text1 = '''\n",
        "A Római Birodalom (latinul Imperium Romanum) az ókori Róma által létrehozott \n",
        "államalakulat volt a Földközi-tenger medencéjében\n",
        "'''\n",
        "\n",
        "text2 = '''\n",
        "Vysoké Tatry sú najvyššie pohorie na Slovensku a v Poľsku a sú zároveň jediným \n",
        "horstvom v týchto štátoch s alpským charakterom. \n",
        "'''\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "dt1 = translator.detect(text1)\n",
        "print(dt1)\n",
        "\n",
        "dt2 = translator.detect(text2)\n",
        "print(dt2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected(lang=en, confidence=1.0)\n",
            "Detected(lang=sk, confidence=1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YNLdvbazJ-O",
        "colab_type": "text"
      },
      "source": [
        "# Translate 107 languages supported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Rp3b9KzM4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "e663cbe5-06cb-44d1-f78e-298f375a85f1"
      },
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "translated = translator.translate(''' Memory Management\n",
        "\n",
        "Memory management refers to management of Primary Memory or Main Memory. Main memory is a large array of words or bytes where each word or byte has its own address.\n",
        "\n",
        "Main memory provides a fast storage that can be accessed directly by the CPU. For a program to be executed, it must in the main memory. An Operating System does the following activities for memory management −\n",
        "\n",
        "    Keeps tracks of primary memory, i.e., what part of it are in use by whom, what part are not in use.\n",
        "\n",
        "    In multiprogramming, the OS decides which process will get memory when and how much.\n",
        "\n",
        "    Allocates the memory when a process requests it to do so.\n",
        "\n",
        "    De-allocates the memory when a process no longer needs it or has been terminated.\n",
        "\n",
        "Processor Management\n",
        "\n",
        "In multiprogramming environment, the OS decides which process gets the processor when and for how much time. This function is called process scheduling. An Operating System does the following activities for processor management −\n",
        "\n",
        "    Keeps tracks of processor and status of process. The program responsible for this task is known as traffic controller.\n",
        "\n",
        "    Allocates the processor (CPU) to a process.\n",
        "\n",
        "    De-allocates processor when a process is no longer required.\n",
        "\n",
        "Device Management\n",
        "\n",
        "An Operating System manages device communication via their respective drivers. It does the following activities for device management −\n",
        "\n",
        "    Keeps tracks of all devices. Program responsible for this task is known as the I/O controller.\n",
        "\n",
        "    Decides which process gets the device when and for how much time.\n",
        "\n",
        "    Allocates the device in the efficient way.\n",
        "\n",
        "    De-allocates devices.\n",
        "''', src='en', dest='hi')\n",
        "\n",
        "print(translated.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "स्मृति प्रबंधन\n",
            "\n",
            "मेमोरी प्रबंधन प्राथमिक मेमोरी या मेन मेमोरी के प्रबंधन को संदर्भित करता है। मुख्य मेमोरी शब्दों या बाइट्स का एक बड़ा सरणी है जहां प्रत्येक शब्द या बाइट का अपना पता होता है।\n",
            "\n",
            "मुख्य मेमोरी एक तेज़ स्टोरेज प्रदान करती है जिसे सीपीयू द्वारा सीधे एक्सेस किया जा सकता है। किसी प्रोग्राम को निष्पादित करने के लिए, यह मुख्य मेमोरी में होना चाहिए। एक ऑपरेटिंग सिस्टम मेमोरी प्रबंधन के लिए निम्नलिखित गतिविधियाँ करता है -\n",
            "\n",
            "    प्राथमिक मेमोरी का ट्रैक रखता है, अर्थात, इसका कौन सा भाग किसके उपयोग में है, कौन सा भाग उपयोग में नहीं है।\n",
            "\n",
            "    मल्टीप्रोग्रामिंग में, ओएस तय करता है कि किस प्रक्रिया को कब और कितना मेमोरी मिलेगी।\n",
            "\n",
            "    स्मृति आवंटित करता है जब कोई प्रक्रिया ऐसा करने का अनुरोध करती है।\n",
            "\n",
            "    जब कोई प्रक्रिया को इसकी आवश्यकता नहीं है या इसे समाप्त कर दिया गया है, तो डी-मेमोरी को आवंटित करता है।\n",
            "\n",
            "प्रोसेसर प्रबंधन\n",
            "\n",
            "मल्टीप्रोग्रामिंग वातावरण में, ओएस तय करता है कि किस प्रक्रिया को प्रोसेसर कब और कितने समय के लिए मिलता है। इस फंक्शन को प्रोसेस शेड्यूलिंग कहा जाता है। एक ऑपरेटिंग सिस्टम प्रोसेसर प्रबंधन के लिए निम्नलिखित गतिविधियाँ करता है -\n",
            "\n",
            "    प्रोसेसर और प्रक्रिया की स्थिति पर नज़र रखता है। इस कार्य के लिए जिम्मेदार कार्यक्रम को ट्रैफिक कंट्रोलर के रूप में जाना जाता है।\n",
            "\n",
            "    प्रोसेसर (सीपीयू) को एक प्रक्रिया में आवंटित करता है।\n",
            "\n",
            "    जब प्रक्रिया की आवश्यकता नहीं रह जाती है, तो प्रोसेसर को डी-आवंटित करता है।\n",
            "\n",
            "डिवाइस प्रबंधन\n",
            "\n",
            "एक ऑपरेटिंग सिस्टम अपने संबंधित ड्राइवरों के माध्यम से डिवाइस संचार का प्रबंधन करता है। यह उपकरण प्रबंधन के लिए निम्नलिखित गतिविधियाँ करता है -\n",
            "\n",
            "    सभी उपकरणों का ट्रैक रखता है। इस कार्य के लिए जिम्मेदार कार्यक्रम I / O नियंत्रक के रूप में जाना जाता है।\n",
            "\n",
            "    निर्णय लेता है कि किस प्रक्रिया को डिवाइस कब और कितने समय के लिए मिलता है।\n",
            "\n",
            "    डिवाइस को कुशल तरीके से आवंटित करता है।\n",
            "\n",
            "    उपकरणों को डी-आवंटित करता है।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlH_L5sFzxu2",
        "colab_type": "text"
      },
      "source": [
        "# Automatic reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3PqQcF1JzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3104581f-513f-4370-97da-3d6fa58bad65"
      },
      "source": [
        "!pip install pyttsx3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyttsx3 in /usr/local/lib/python3.6/dist-packages (2.90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJCQgwwLzjNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the pyttsx library \n",
        "import pyttsx3 \n",
        "\n",
        "# initialisation \n",
        "engine = pyttsx3.init() \n",
        "\n",
        "# testing \n",
        "#engine.say(\"My first code on text-to-speech\") \n",
        "engine.say(\"Lets operate\") \n",
        "engine.runAndWait() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ_W4Hx01Rzf",
        "colab_type": "text"
      },
      "source": [
        "## Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOwQRmrN4I3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "03e162dc-bad1-4bb3-82c5-446fe9e9baf7"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = 'https://www.tutorialspoint.com/operating_system/os_overview.htm'\n",
        "page = requests.get(URL)\n",
        "content=\"\"\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "p = soup.find_all('p')\n",
        "for text in p:\n",
        "  content+=text.get_text()\n",
        "print(content)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An Operating System (OS) is an interface between a computer user and computer hardware. An operating system is a software which performs all the basic tasks like file management, memory management, process management, handling input and output, and controlling peripheral devices such as disk drives and printers.Some popular Operating Systems include Linux Operating System, Windows Operating System, VMS, OS/400, AIX, z/OS, etc.An operating system is a program that acts as an interface between the user and the computer hardware and controls the execution of all kinds of programs.Following are some of important functions of an operating System.Memory management refers to management of Primary Memory or Main Memory. Main memory is a large array of words or bytes where each word or byte has its own address.Main memory provides a fast storage that can be accessed directly by the CPU. For a program to be executed, it must in the main memory. An Operating System does the following activities for memory management −Keeps tracks of primary memory, i.e., what part of it are in use by whom, what part are not in use.In multiprogramming, the OS decides which process will get memory when and how much.Allocates the memory when a process requests it to do so.De-allocates the memory when a process no longer needs it or has been terminated.In multiprogramming environment, the OS decides which process gets the processor when and for how much time. This function is called process  scheduling. An Operating System does the following activities for processor management −Keeps tracks of processor and status of process. The program responsible for this task is known as traffic controller.Allocates the processor (CPU) to a process.De-allocates processor when a process is no longer required.An Operating System manages device communication via their respective drivers. It does the following activities for device management −Keeps tracks of all devices. Program responsible for this task is known as the I/O controller.Decides which process gets the device when and for how much time.Allocates the device in the efficient way.De-allocates devices.A file system is normally organized into directories for easy navigation and usage. These directories may contain files and other directions.An Operating System does the following activities for file management −Keeps track of information, location, uses, status etc. The collective facilities are often known as file system.Decides who gets the resources.Allocates the resources.De-allocates the resources.Following are some of the important activities that an Operating System performs −Security −  By means of password and similar other techniques, it prevents unauthorized access to programs and data.Control over system performance − Recording delays between request for a service and response from the system.Job accounting − Keeping track of time and resources used by various jobs and users.Error detecting aids − Production of dumps, traces, error messages, and other debugging and error detecting aids.Coordination between other softwares and users − Coordination and assignment of compilers, interpreters, assemblers and other software to the various users of the computer systems.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX8ReEAk6RAR",
        "colab_type": "text"
      },
      "source": [
        "## Read text from pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIOIBuAJHQqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbUkjBX6OUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cab34df7-9f2a-495b-e91f-9a0997d8f7ff"
      },
      "source": [
        "import PyPDF2  \n",
        "    \n",
        "# creating a pdf file object  \n",
        "pdfFileObj = open('/content/CV.pdf', 'rb')  \n",
        "    \n",
        "# creating a pdf reader object  \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)  \n",
        "    \n",
        "# printing number of pages in pdf file  \n",
        "print(pdfReader.numPages)  \n",
        "    \n",
        "# creating a page object  \n",
        "pageObj = pdfReader.getPage(1)  \n",
        "    \n",
        "# extracting text from page  \n",
        "print(pageObj.extractText())  \n",
        "    \n",
        "# closing the pdf file object  \n",
        "pdfFileObj.close()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "˝˝ ˝˜0˚˜˜˜\n",
            "! \n",
            "!\"-.˙\n",
            "\n",
            "9˝\n",
            "ˇˇ˙+˘ \n",
            "=@˚˜˝˜˚\n",
            "˚˜˝˜˜\n",
            "0˝˝˜˜˝˜\n",
            "\n",
            "˙\n",
            "\n",
            "9˝\n",
            "ˇˇ\n",
            "=',,\n",
            "=˜˝˜˝\n",
            "0˚˜\n",
            "                                                                                                           C\n",
            "; %-#˝7˝˜˝#˝\n",
            "0\n",
            "%˜2DD˜˝\" \n",
            "˝\n",
            "˜˝7;,%˜˝\n",
            "E˝\n",
            "67;,%˜˝\n",
            "A\"2\n",
            "                                                                                                           C1ˇ\n",
            ":˜˜˝\n",
            "\n",
            "˝G\n",
            "˜˝˜˝#˝00˜\n",
            "\n",
            "6\"H.˜\n",
            "˙ˆ\n",
            "ˇ\n",
            "˛\n",
            "A\n",
            "/˝\n",
            "4ˇ\n",
            "B\n",
            "5˜\n",
            "˙ˇ\n",
            "5˙ˆ3ˇ\n",
            ":(9@I44%%\n",
            "\n",
            "/,J''˜˝;\n",
            "4\n",
            "<='A,,K'4.,!˜-˝\n",
            "                                                                                                 Cˇ\n",
            "\"\n",
            "5˜˝˝\n",
            ",(˜\n",
            "20˝\n",
            "-˜˙˜˜˝/\n",
            "J#˜˜˝-˜\n",
            "˙˜˜˝/\n",
            "2!˝˜*˜\n",
            "#˝*0!#\n",
            "\"˜˚(˜\n",
            "                                                                                              CC \n",
            "05;*)%=2DD˝G\n",
            "0˜˝˜\n",
            "9˜˜7\n",
            "* \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmHNVyZBlvlP",
        "colab_type": "text"
      },
      "source": [
        "# Question Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psAkW4stl0Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjarROPOl8-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4N1exYcmztj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4e6c8593-4319-4d57-bfef-f420b7eac76e"
      },
      "source": [
        "!pip install unicode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unicode\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/e8/d30276462ce627d1fd2c547cdf64a7eccec20e86f4fcaa145e1ea8758641/unicode-2.7-py2.py3-none-any.whl\n",
            "Installing collected packages: unicode\n",
            "Successfully installed unicode-2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqFq_1NimB5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwnaikoEnZct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "if sys.version_info[0] >= 3:\n",
        "    unicode = str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUT78F4qmF6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ww2 = '''\n",
        "An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.\n",
        "\n",
        "Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.\n",
        "\n",
        "For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer – from cellular phones and video game consoles to web servers and supercomputers.\n",
        "\n",
        "The dominant desktop operating system is Microsoft Windows with a market share of around 82.74%. macOS by Apple Inc. is in second place (13.23%), and the varieties of Linux are collectively in third place (1.57%).[3] In the mobile sector (including smartphones and tablets), Android's share is up to 70% in the year 2017.[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.[5] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications. \n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AO5A6RYnqVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ww2b = TextBlob(st)\n",
        "sposs = {}\n",
        "sentences=ww2b.split(\"/.\")\n",
        "for sentence in ww2b.sentences:\n",
        "    \n",
        "    # We are going to prepare the dictionary of parts-of-speech as the key and value is a list of words:\n",
        "    # {part-of-speech: [word1, word2]}\n",
        "    # We are basically grouping the words based on the parts-of-speech\n",
        "    poss = {}\n",
        "    sposs[sentence.string] = poss;\n",
        "    for t in sentence.tags:\n",
        "        tag = t[1]\n",
        "        if tag not in poss:\n",
        "            poss[tag] = []\n",
        "        poss[tag].append(t[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IkVT1fwpNSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sposs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-U7Zc26nu4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "# Create the blank in string\n",
        "def replaceIC(word, sentence):\n",
        "    insensitive_hippo = re.compile(re.escape(word), re.IGNORECASE)\n",
        "    return insensitive_hippo.sub('__________________', sentence)\n",
        "\n",
        "# For a sentence create a blank space.\n",
        "# It first tries to randomly selection proper-noun \n",
        "# and if the proper noun is not found, it selects a noun randomly.\n",
        "def removeWord(sentence, poss):\n",
        "    words =\"\"\n",
        "    imp_words=[]\n",
        "    if 'NNP' in poss:\n",
        "       imp_words.append(poss['NNP'])\n",
        "    if 'NN' in poss:\n",
        "        imp_words.append(poss['NN'])\n",
        "    if 'JJ' in poss:\n",
        "        imp_words.append(poss['JJ'])\n",
        "    if 'VB' in poss:\n",
        "        \n",
        "        imp_words.append(poss['VB'])\n",
        "    #else:\n",
        "     #   print(\"NN and NNP not found\")\n",
        "      #  return (None, sentence, None)\n",
        "    words = max(imp_words, key = len) \n",
        "   \n",
        "    print(words)\n",
        "    if len(words) > 0:\n",
        "        word = random.choice(words)\n",
        "        replaced = replaceIC(word, sentence)\n",
        "        return (word, sentence, replaced)\n",
        "    else:\n",
        "        print(\"words are empty\")\n",
        "        return (None, sentence, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-oBBnzFJUTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st='''An operating system is system software that manages computer hardware, software resources, and provides common services for computer programs.\n",
        "Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.\n",
        "For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it.\n",
        " According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.\n",
        "Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNipL9lknyk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "2897c7e8-da86-4475-d270-6d5482ce37e6"
      },
      "source": [
        "# Iterate over the sentenses \n",
        "for sentence in sposs.keys():\n",
        "    poss = sposs[sentence]\n",
        "    (word, osentence, replaced) = removeWord(sentence, poss)\n",
        "    if replaced is None:\n",
        "        print (\"Founded none for \")\n",
        "        print(sentence)\n",
        "    else:\n",
        "        \n",
        "        print(replaced)\n",
        "        print (\"\\n===============\")\n",
        "        print(word)\n",
        "        print (\"===============\")\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['operating', 'system', 'system', 'software', 'computer', 'hardware', 'software', 'computer']\n",
            "An operating system is system __________________ that manages computer hardware, __________________ resources, and provides common services for computer programs.\n",
            "\n",
            "===============\n",
            "software\n",
            "===============\n",
            "\n",
            "\n",
            "['schedule', 'use', 'system', 'accounting', 'software', 'cost', 'allocation', 'processor', 'time', 'mass', 'storage', 'printing']\n",
            "Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, __________________, and other resources.\n",
            "\n",
            "===============\n",
            "printing\n",
            "===============\n",
            "\n",
            "\n",
            "['hardware', 'input', 'output', 'memory', 'allocation', 'operating', 'system', 'computer', 'hardware', 'application', 'code', 'hardware', 'system', 'function']\n",
            "For __________________ functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer __________________,[1][2] although the application code is usually executed directly by the __________________ and frequently makes system calls to an OS function or is interrupted by it.\n",
            "\n",
            "===============\n",
            "hardware\n",
            "===============\n",
            "\n",
            "\n",
            "['quarter', 'data', 'share', 'percent', 'growth', 'rate', 'percent', 'year', 'iOS', 'percent', 'year', 'decrease', 'market', 'share', 'percent', 'amount', 'percent']\n",
            "According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per __________________, followed by Apple's iOS with 12.1 percent with per __________________ decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.\n",
            "\n",
            "===============\n",
            "year\n",
            "===============\n",
            "\n",
            "\n",
            "['Other', 'specialized', 'such', 'embedded', 'real-time', 'many']\n",
            "Other __________________ classes of operating systems, such as embedded and real-time systems, exist for many applications.\n",
            "\n",
            "===============\n",
            "specialized\n",
            "===============\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHbhj7jnq_03",
        "colab_type": "text"
      },
      "source": [
        "# Summarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFjREjv5rG4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sumy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X08QTeM7r0md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r3Q5n23sNOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M80WF8hVs3Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_nPeMwYuw0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYf8S4-Gt2g_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1531f052-2ce4-45cd-ee8b-571132735c87"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer as Summarizer\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "\n",
        "LANGUAGE = \"english\"\n",
        "SENTENCES_COUNT = 5\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "   \n",
        "    parser = PlaintextParser.from_string(ww2, Tokenizer(LANGUAGE))\n",
        "    stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "    summarizer = Summarizer(stemmer)\n",
        "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "    test=\"\"\n",
        "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "        test+=str(sentence)\n",
        "        print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.\n",
            "Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.\n",
            "For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it.\n",
            "[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.\n",
            "Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyZgpvDoxsn1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d16eab69-304c-4c95-9e2d-8be8e07ca815"
      },
      "source": [
        "import re\n",
        "text='''An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.\n",
        "Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.\n",
        "For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it.\n",
        "[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.\n",
        "Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.\n",
        "'''\n",
        "st=re.sub(r'[^A-Za-z0-9 /.\\n]+', '', text)\n",
        "print(st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An operating system OS is system software that manages computer hardware software resources and provides common services for computer programs.\n",
            "Timesharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time mass storage printing and other resources.\n",
            "For hardware functions such as input and output and memory allocation the operating system acts as an intermediary between programs and the computer hardware12 although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it.\n",
            "4 According to third quarter 2016 data Androids share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year followed by Apples iOS with 12.1 percent with per year decrease in market share of 5.2 percent while other operating systems amount to just 0.3 percent.\n",
            "Other specialized classes of operating systems such as embedded and realtime systems exist for many applications.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thD7xCYA2q_w",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7nTSAYc4B3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "92eb607f-a289-4f78-f070-0d0b84213f04"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzwnNkgn2vTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "b9861759-8075-4587-a6c5-3a4864fdbe3f"
      },
      "source": [
        "aqg = AutomaticQuestionGenerator()\n",
        "\n",
        "#inputTextPath = \"input file path -- ?? ../DB/db.txt\"\n",
        "#readFile = open(inputTextPath, 'r+', encoding=\"utf8\")\n",
        "#readFile = open(inputTextPath, 'r+', encoding=\"utf8\", errors = 'ignore')\n",
        "\n",
        "#inputText = readFile.read()\n",
        "inputText = '''\n",
        "Time sharing operating systems schedule tasks for efficient use of the system\n",
        "'''\n",
        "text ='''Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \\\n",
        "and first released in 1991, Python's design philosophy emphasizes code \\\n",
        "readability with its notable use of significant whitespace.'''\n",
        "\n",
        "text2 = '''Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all \\\n",
        "things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
        "one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. \\\n",
        "The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
        "and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of \\\n",
        "the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
        "weaker as objects get further away'''\n",
        "questionList = aqg.aqgParse(text)\n",
        "display(questionList)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1895d8b1189e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'''Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly weaker as objects get further away'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mquestionList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maqg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maqgParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestionList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-eefd98632ab7>\u001b[0m in \u001b[0;36maqgParse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     30\u001b[0m                             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mclause_identify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentSets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                 \u001b[0mquestionsList\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwhom_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentSets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-57ed148faa89>\u001b[0m in \u001b[0;36mclause_identify\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclause_identify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?|VB.?|MD|RP>+}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mchunkparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCzI2C93fVL",
        "colab_type": "text"
      },
      "source": [
        "####aggfunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ2M1ykY3YZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import en_core_web_md\n",
        "#import clause\n",
        "#import nonClause\n",
        "#import identification\n",
        "#import questionValidation\n",
        "#from nlpNER import nerTagger\n",
        "\n",
        "\n",
        "class AutomaticQuestionGenerator():\n",
        "    # AQG Parsing & Generate a question\n",
        "    def aqgParse(self, sentence):\n",
        "\n",
        "        nlp = spacy.load(\"en\")\n",
        "        nlp = en_core_web_md.load()\n",
        "\n",
        "        singleSentences = sentence.split(\".\")\n",
        "        questionsList = []\n",
        "        if len(singleSentences) != 0:\n",
        "            for i in range(len(singleSentences)):\n",
        "                segmentSets = singleSentences[i].split(\",\")\n",
        "\n",
        "                ner = nerTagger(nlp, singleSentences[i])\n",
        "\n",
        "                if (len(segmentSets)) != 0:\n",
        "                    for j in range(len(segmentSets)):\n",
        "                        try:\n",
        "                            questionsList += howmuch_2(segmentSets, j, ner)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                        if clause_identify(segmentSets[j]) == 1:\n",
        "                            try:\n",
        "                                questionsList += whom_1(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList +=whom_2(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList +=whom_3(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList +=whose(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList += what_to_do(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList += who(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList +=howmuch_1(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                            try:\n",
        "                                questionsList += howmuch_3(segmentSets, j, ner)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "\n",
        "\n",
        "                            else:\n",
        "                                #try:\n",
        "                                s = subjectphrase_search(segmentSets, j)\n",
        "                                #except Exception:\n",
        "                                    #pass\n",
        "\n",
        "                                if len(s) != 0:\n",
        "                                    segmentSets[j] = s + segmentSets[j]\n",
        "                                    try:\n",
        "                                        questionsList += whom_1(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                                    try:\n",
        "                                        questionsList += whom_2(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                                    try:\n",
        "                                        questionsList += whom_3(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                                    try:\n",
        "                                        questionsList += whose(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                                    try:\n",
        "                                        questionsList += what_to_do(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "                                    try:\n",
        "                                        questionsList += who(segmentSets, j, ner)\n",
        "                                    except Exception:\n",
        "                                        pass\n",
        "\n",
        "                                    else:\n",
        "                                        try:\n",
        "                                            questionsList += what_whom1(segmentSets, j, ner)\n",
        "                                        except Exception:\n",
        "                                            pass\n",
        "                                        try:\n",
        "                                            questionsList +=what_whom2(segmentSets, j, ner)\n",
        "                                        except Exception:\n",
        "                                            pass\n",
        "                                        try:\n",
        "                                            questionsList += whose(segmentSets, j, ner)\n",
        "                                        except Exception:\n",
        "                                            pass\n",
        "                                        try:\n",
        "                                            questionsList +=howmany(segmentSets, j, ner)\n",
        "                                        except Exception:\n",
        "                                            pass\n",
        "                                        try:\n",
        "                                            questionsList += howmuch_1(segmentSets, j, ner)\n",
        "                                        except Exception:\n",
        "                                            pass\n",
        "\n",
        "                questionsList.append('\\n')\n",
        "        return questionsList\n",
        "\n",
        "\n",
        "\n",
        "    def DisNormal(self, str):\n",
        "        print(\"\\n\")\n",
        "        print(\"------X------\")\n",
        "        print(\"Start  output:\\n\")\n",
        "\n",
        "        count = 0\n",
        "        out = \"\"\n",
        "\n",
        "        for i in range(len(str)):\n",
        "            count = count + 1\n",
        "            print(\"Q-0%d: %s\" % (count, str[i]))\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"End  OutPut\")\n",
        "        print(\"-----X-----\\n\\n\")\n",
        "\n",
        "\n",
        "    # AQG Display the Generated Question\n",
        "    def display(self, str):\n",
        "        print(\"\\n\")\n",
        "        print(\"------X------\")\n",
        "        print(\"Start  output:\\n\")\n",
        "\n",
        "        count = 0\n",
        "        out = \"\"\n",
        "        for i in range(len(str)):\n",
        "            if (len(str[i]) >= 3):\n",
        "                if (hNvalidation(str[i]) == 1):\n",
        "                    if ((str[i][0] == 'W' and str[i][1] == 'h') or (str[i][0] == 'H' and str[i][1] == 'o') or (\n",
        "                            str[i][0] == 'H' and str[i][1] == 'a')):\n",
        "                        WH = str[i].split(',')\n",
        "                        if (len(WH) == 1):\n",
        "                            str[i] = str[i][:-1]\n",
        "                            str[i] = str[i][:-1]\n",
        "                            str[i] = str[i][:-1]\n",
        "                            str[i] = str[i] + \"?\"\n",
        "                            count = count + 1\n",
        "\n",
        "                            if (count < 10):\n",
        "                                print(\"Q-0%d: %s\" % (count, str[i]))\n",
        "                                out += \"Q-0\" + count.__str__() + \": \" + str[i] + \"\\n\"\n",
        "\n",
        "                            else:\n",
        "                                print(\"Q-%d: %s\" % (count, str[i]))\n",
        "                                out += \"Q-\" + count.__str__() + \": \" + str[i] + \"\\n\"\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"End  OutPut\")\n",
        "        print(\"-----X-----\\n\\n\")\n",
        "\n",
        "        output = \"output file path -- ?? ../DB/output.txt\"\n",
        "        w = open(output, 'w+', encoding=\"utf8\")\n",
        "        w.write(out)\n",
        "        w.close()\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGBLlP31gUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4dgXw63kPd",
        "colab_type": "text"
      },
      "source": [
        "####clause"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gl5oKww3l2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "#import identification\n",
        "#import nonClause\n",
        "\n",
        "\n",
        "def whom_1(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 +=get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[j][1][1] == 'PRP':\n",
        "                    str2 = \" to whom \"\n",
        "                else:\n",
        "                    for x in range(len(chunked[j])):\n",
        "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                chunked[j][x][1] == \"NN\"):\n",
        "                            break\n",
        "\n",
        "                    for x1 in range(len(ner)):\n",
        "\n",
        "                        if ner[x1][0] == chunked[j][x][0]:\n",
        "                            if ner[x1][1] == \"PERSON\":\n",
        "                                str2 = \" to whom \"\n",
        "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                str2 = \" where \"\n",
        "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                str2 = \" when \"\n",
        "                            else:\n",
        "                                str2 = \"to what \"\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    st = str5 + str2 + str4 + str6 + str3\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def whom_2(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[j][1][1] == 'PRP':\n",
        "                    str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
        "                else:\n",
        "                    for x in range(len(chunked[j])):\n",
        "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                chunked[j][x][1] == \"NN\"):\n",
        "                            break\n",
        "\n",
        "                    for x1 in range(len(ner)):\n",
        "                        if ner[x1][0] == chunked[j][x][0]:\n",
        "                            if ner[x1][1] == \"PERSON\":\n",
        "                                str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
        "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                str2 = \" where \"\n",
        "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                str2 = \" when \"\n",
        "                            else:\n",
        "                                str2 = \" \" + chunked[j][0][0] + \" what \"\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    st = str5 + str2 + str4 + str6 + str3\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def whom_3(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<VB.?|MD|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[j][1][1] == 'PRP':\n",
        "                    str2 = \" whom \"\n",
        "                else:\n",
        "                    for x in range(len(chunked[j])):\n",
        "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                chunked[j][x][1] == \"NN\"):\n",
        "                            break\n",
        "\n",
        "                    for x1 in range(len(ner)):\n",
        "                        if ner[x1][0] == chunked[j][x][0]:\n",
        "                            if ner[x1][1] == \"PERSON\":\n",
        "                                str2 = \" whom \"\n",
        "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                str2 = \" what \"\n",
        "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                str2 = \" what time \"\n",
        "                            else:\n",
        "                                str2 = \" what \"\n",
        "\n",
        "                strx = get_chunk(chunked[j])\n",
        "                tok = nltk.word_tokenize(strx)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<VB.?|MD>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                strx = get_chunk(chunked1[0])\n",
        "\n",
        "                str1 += strx\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    st = str5 + str2 + str4 + str6 + str3\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def whose(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<DT|NN.?>*<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for i in range(len(chunked)):\n",
        "            if i in list1:\n",
        "                str1 = \"\"\n",
        "                str3 = \"\"\n",
        "                str2 = \"\"\n",
        "                for k in range(i):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                str1 += \" whose \"\n",
        "\n",
        "                for k in range(i + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[i][1][1] == 'POS':\n",
        "                    for k in range(2, len(chunked[i])):\n",
        "                        str2 += (chunked[i][k][0] + \" \")\n",
        "\n",
        "                if chunked[i][0][1] == 'PRP$':\n",
        "                    for k in range(1, len(chunked[i])):\n",
        "                        str2 += (chunked[i][k][0] + \" \")\n",
        "\n",
        "                str2 = str1 + str2 + str3\n",
        "                str4 = \"\"\n",
        "\n",
        "                for l in range(0, len(segment_set)):\n",
        "                    if l < num:\n",
        "                        str4 += (segment_set[l] + \",\")\n",
        "                    if l > num:\n",
        "                        str2 += (\",\" + segment_set[l])\n",
        "                str2 = str4 + str2\n",
        "                str2 += '?'\n",
        "                str2 = postprocess(str2)\n",
        "                # str2 = 'Q.' + str2\n",
        "                list3.append(str2)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def what_to_do(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<TO>+<VB|VBP|RP>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>*}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                ls = get_chunk(chunked[j])\n",
        "                tok = nltk.word_tokenize(ls)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked2 = chunkparser.parse(tag)\n",
        "                lis = chunk_search(ls, chunked2)\n",
        "                if len(lis) != 0:\n",
        "                    x = lis[len(lis) - 1]\n",
        "                    ls1 = get_chunk(chunked2[x])\n",
        "                    index = ls.find(ls1)\n",
        "                    str2 = \" \" + ls[0:index]\n",
        "                else:\n",
        "                    str2 = \" to do \"\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    if chunked2[j][1][1] == 'PRP':\n",
        "                        tr = \" whom \"\n",
        "                    else:\n",
        "                        for x in range(len(chunked[j])):\n",
        "                            if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                    chunked[j][x][1] == \"NN\"):\n",
        "                                break\n",
        "\n",
        "                        for x1 in range(len(ner)):\n",
        "                            if ner[x1][0] == chunked[j][x][0]:\n",
        "                                if ner[x1][1] == \"PERSON\":\n",
        "                                    tr = \" whom \"\n",
        "                                elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                    tr = \" where \"\n",
        "                                elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                    tr = \" when \"\n",
        "                                else:\n",
        "                                    tr = \" what \"\n",
        "\n",
        "                    st = str5 + tr + str4 + str2 + str6 + str3\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def who(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(list1)):\n",
        "            m = list1[j]\n",
        "            str1 = \"\"\n",
        "            for k in range(m + 1, len(chunked)):\n",
        "                if k in list1:\n",
        "                    str1 += get_chunk(chunked[k])\n",
        "                else:\n",
        "                    str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "            str2 = get_chunk(chunked[m])\n",
        "            tok = nltk.word_tokenize(str2)\n",
        "            tag = nltk.pos_tag(tok)\n",
        "\n",
        "            for m11 in range(len(tag)):\n",
        "                if tag[m11][1] == 'NNP' or tag[m11][1] == 'NNPS' or tag[m11][1] == 'NNS' or tag[m11][1] == 'NN':\n",
        "                    break\n",
        "            s11 = ' who '\n",
        "            for m12 in range(len(ner)):\n",
        "                if ner[m12][0] == tag[m11][0]:\n",
        "                    if ner[m12][1] == 'LOC':\n",
        "                        s11 = ' which place '\n",
        "                    elif ner[m12][1] == 'ORG':\n",
        "                        s11 = ' who '\n",
        "                    elif ner[m12][1] == 'DATE' or ner[m12][1] == 'TIME':\n",
        "                        s11 = ' what time '\n",
        "                    else:\n",
        "                        s11 = ' who '\n",
        "\n",
        "            gram = r\"\"\"chunk:{<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "            chunkparser = nltk.RegexpParser(gram)\n",
        "            chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "            list2 = chunk_search(str2, chunked1)\n",
        "            if len(list2) != 0:\n",
        "                str2 = get_chunk(chunked1[list2[0]])\n",
        "                str2 = s11 + str2\n",
        "                for k in range(list2[0] + 1, len(chunked1)):\n",
        "                    if k in list2:\n",
        "                        str2 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str2 += (chunked[k][0] + \" \")\n",
        "                str2 += (\" \" + str1)\n",
        "\n",
        "                tok_1 = nltk.word_tokenize(str2)\n",
        "                str2 = \"\"\n",
        "                for h in range(len(tok_1)):\n",
        "                    if tok_1[h] == \"am\":\n",
        "                        str2 += \" is \"\n",
        "                    else:\n",
        "                        str2 += (tok_1[h] + \" \")\n",
        "\n",
        "                for l in range(num + 1, len(segment_set)):\n",
        "                    str2 += (\",\" + segment_set[l])\n",
        "                str2 += '?'\n",
        "\n",
        "                str2 = postprocess(str2)\n",
        "                # str2 = 'Q.' + str2\n",
        "                list3.append(str2)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def howmuch_2(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<\\$>*<CD>+<MD>?<VB|VBD|VBG|VBP|VBN|VBZ|RP>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(list1)):\n",
        "            m = list1[j]\n",
        "            str1 = \"\"\n",
        "            for k in range(m + 1, len(chunked)):\n",
        "                if k in list1:\n",
        "                    str1 += get_chunk(chunked[k])\n",
        "                else:\n",
        "                    str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "            str2 = get_chunk(chunked[m])\n",
        "            tok = nltk.word_tokenize(str2)\n",
        "            tag = nltk.pos_tag(tok)\n",
        "            gram = r\"\"\"chunk:{<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "            chunkparser = nltk.RegexpParser(gram)\n",
        "            chunked1 = chunkparser.parse(tag)\n",
        "            s11 = ' how much '\n",
        "\n",
        "            list2 = chunk_search(str2, chunked1)\n",
        "            if len(list2) != 0:\n",
        "                str2 = get_chunk(chunked1[list2[0]])\n",
        "                str2 = s11 + str2\n",
        "                for k in range(list2[0] + 1, len(chunked1)):\n",
        "                    if k in list2:\n",
        "                        str2 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str2 += (chunked[k][0] + \" \")\n",
        "                str2 += (\" \" + str1)\n",
        "\n",
        "                tok_1 = nltk.word_tokenize(str2)\n",
        "                str2 = \"\"\n",
        "                for h in range(len(tok_1)):\n",
        "                    if tok_1[h] == \"am\":\n",
        "                        str2 += \" is \"\n",
        "                    else:\n",
        "                        str2 += (tok_1[h] + \" \")\n",
        "\n",
        "                for l in range(num + 1, len(segment_set)):\n",
        "                    str2 += (\",\" + segment_set[l])\n",
        "                str2 += '?'\n",
        "\n",
        "                str2 = postprocess(str2)\n",
        "                # str2 = 'Q.' + str2\n",
        "                list3.append(str2)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def howmuch_1(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<IN>+<\\$>?<CD>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                str2 = ' ' + chunked[j][0][0] + ' how much '\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    st = str5 + str2 + str4 + str6 + str3\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3\n",
        "\n",
        "\n",
        "def howmuch_3(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN|TO>?<PRP|PRP\\$|NN.?>?<\\$>*<CD>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    list3 = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str2 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                strx = get_chunk(chunked[j])\n",
        "                tok = nltk.word_tokenize(strx)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<MD>?<VB|VBD|VBG|VBP|VBN|VBZ>+<IN|TO>?<PRP|PRP\\$|NN.?>?}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                strx = get_chunk(chunked1[0])\n",
        "                str1 += (\" \" + strx)\n",
        "\n",
        "                str2 = ' how much '\n",
        "\n",
        "                tok = nltk.word_tokenize(str1)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(str1, chunked1)\n",
        "\n",
        "                if len(list2) != 0:\n",
        "                    m = list2[len(list2) - 1]\n",
        "\n",
        "                    str4 = get_chunk(chunked1[m])\n",
        "                    str4 = verbphrase_identify(str4)\n",
        "                    str5 = \"\"\n",
        "                    str6 = \"\"\n",
        "\n",
        "                    for k in range(m):\n",
        "                        if k in list2:\n",
        "                            str5 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str5 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    for k in range(m + 1, len(chunked1)):\n",
        "                        if k in list2:\n",
        "                            str6 += get_chunk(chunked1[k])\n",
        "                        else:\n",
        "                            str6 += (chunked1[k][0] + \" \")\n",
        "\n",
        "                    st = str5 + str2 + str4 + str6 + str3\n",
        "\n",
        "                    for l in range(num + 1, len(segment_set)):\n",
        "                        st += (\",\" + segment_set[l])\n",
        "                    st += '?'\n",
        "                    st = postprocess(st)\n",
        "                    # st = 'Q.' + st\n",
        "                    list3.append(st)\n",
        "\n",
        "    return list3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIswfzVE4Ozm",
        "colab_type": "text"
      },
      "source": [
        "#####non clause"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38T1Tjhr4RRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "#import identification\n",
        "\n",
        "\n",
        "def get_chunk(chunked):\n",
        "    str1 = \"\"\n",
        "    for j in range(len(chunked)):\n",
        "        str1 += (chunked[j][0] + \" \")\n",
        "    return str1\n",
        "\n",
        "def what_whom1(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<TO>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|VBG|DT|POS|CD|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    s = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[j][1][1] == 'PRP':\n",
        "                    str2 = \"to whom \"\n",
        "                else:\n",
        "                    for x in range(len(chunked[j])):\n",
        "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                chunked[j][x][1] == \"NN\"):\n",
        "                            break\n",
        "\n",
        "                    for x1 in range(len(ner)):\n",
        "                        if ner[x1][0] == chunked[j][x][0]:\n",
        "                            if ner[x1][1] == \"PERSON\":\n",
        "                                str2 = \" to whom \"\n",
        "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                str2 = \" where \"\n",
        "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                str2 = \" when \"\n",
        "                            else:\n",
        "                                str2 = \"to what\"\n",
        "\n",
        "                str4 = str1 + str2 + str3\n",
        "                for k in range(len(segment_set)):\n",
        "                    if k != num:\n",
        "                        str4 += (\",\" + segment_set[k])\n",
        "                str4 += '?'\n",
        "                str4 = postprocess(str4)\n",
        "                # str4 = 'Q.' + str4\n",
        "                s.append(str4)\n",
        "    return s\n",
        "\n",
        "\n",
        "def what_whom2(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<IN>+<DT>?<RB.?>*<JJ.?>*<NN.?|PRP|PRP\\$|POS|VBG|DT|CD|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    s = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str3 = \"\"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                if chunked[j][1][1] == 'PRP':\n",
        "                    str2 = \" \" + chunked[j][0][0] + \" whom \"\n",
        "                else:\n",
        "                    for x in range(len(chunked[j])):\n",
        "                        if (chunked[j][x][1] == \"NNP\" or chunked[j][x][1] == \"NNPS\" or chunked[j][x][1] == \"NNS\" or\n",
        "                                chunked[j][x][1] == \"NN\"):\n",
        "                            break\n",
        "\n",
        "                    for x1 in range(len(ner)):\n",
        "                        if ner[x1][0] == chunked[j][x][0]:\n",
        "                            if ner[x1][1] == \"PERSON\":\n",
        "                                str2 = \" \" + chunked[j][0][0] + \"whom \"\n",
        "                            elif ner[x1][1] == \"LOC\" or ner[x1][1] == \"ORG\" or ner[x1][1] == \"GPE\":\n",
        "                                str2 = \" where \"\n",
        "                            elif ner[x1][1] == \"TIME\" or ner[x1][1] == \"DATE\":\n",
        "                                str2 = \" when \"\n",
        "                            else:\n",
        "                                str2 = \" \" + chunked[j][0][0] + \" what\"\n",
        "\n",
        "                str4 = str1 + str2 + str3\n",
        "                for k in range(len(segment_set)):\n",
        "                    if k != num:\n",
        "                        str4 += (\",\" + segment_set[k])\n",
        "                str4 += '?'\n",
        "                str4 = postprocess(str4)\n",
        "                # str4 = 'Q.' + str4\n",
        "                s.append(str4)\n",
        "    return s\n",
        "\n",
        "\n",
        "def whose(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<NN.?>*<PRP\\$|POS>+<RB.?>*<JJ.?>*<NN.?|VBG|VBN>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    s = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str3 = \"\"\n",
        "            str2 = \" whose \"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "                if chunked[j][1][1] == 'POS':\n",
        "                    for k in range(2, len(chunked[j])):\n",
        "                        str2 += (chunked[j][k][0] + \" \")\n",
        "                else:\n",
        "                    for k in range(1, len(chunked[j])):\n",
        "                        str2 += (chunked[j][k][0] + \" \")\n",
        "\n",
        "                str4 = str1 + str2 + str3\n",
        "                for k in range(len(segment_set)):\n",
        "                    if k != num:\n",
        "                        str4 += (\",\" + segment_set[k])\n",
        "                str4 += '?'\n",
        "                str4 = postprocess(str4)\n",
        "                # str4 = 'Q.' + str4\n",
        "                s.append(str4)\n",
        "    return s\n",
        "\n",
        "\n",
        "def howmany(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<DT>?<CD>+<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    s = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str3 = \"\"\n",
        "            str2 = \" how many \"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                st = get_chunk(chunked[j])\n",
        "                tok = nltk.word_tokenize(st)\n",
        "                tag = nltk.pos_tag(tok)\n",
        "                gram = r\"\"\"chunk:{<RB>?<JJ|JJR|JJS>?<NN|NNS|NNP|NNPS|VBG>+}\"\"\"\n",
        "                chunkparser = nltk.RegexpParser(gram)\n",
        "                chunked1 = chunkparser.parse(tag)\n",
        "\n",
        "                list2 = chunk_search(st, chunked1)\n",
        "                z = \"\"\n",
        "\n",
        "                for k in range(len(chunked1)):\n",
        "                    if k in list2:\n",
        "                        z += get_chunk(chunked1[k])\n",
        "\n",
        "                str4 = str1 + str2 + z + str3\n",
        "                for k in range(len(segment_set)):\n",
        "                    if k != num:\n",
        "                        str4 += (\",\" + segment_set[k])\n",
        "                str4 += '?'\n",
        "                str4 = postprocess(str4)\n",
        "                # str4 = 'Q.' + str4\n",
        "                s.append(str4)\n",
        "    return s\n",
        "\n",
        "\n",
        "def howmuch_1(segment_set, num, ner):\n",
        "    tok = nltk.word_tokenize(segment_set[num])\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<IN>+<\\$>?<CD>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    list1 = chunk_search(segment_set[num], chunked)\n",
        "    s = []\n",
        "\n",
        "    if len(list1) != 0:\n",
        "        for j in range(len(chunked)):\n",
        "            str1 = \"\"\n",
        "            str3 = \"\"\n",
        "            str2 = \" how much \"\n",
        "            if j in list1:\n",
        "                for k in range(j):\n",
        "                    if k in list1:\n",
        "                        str1 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str1 += (chunked[k][0] + \" \")\n",
        "                for k in range(j + 1, len(chunked)):\n",
        "                    if k in list1:\n",
        "                        str3 += get_chunk(chunked[k])\n",
        "                    else:\n",
        "                        str3 += (chunked[k][0] + \" \")\n",
        "\n",
        "                str2 = chunked[j][0][0] + str2\n",
        "                str4 = str1 + str2 + str3\n",
        "                for k in range(len(segment_set)):\n",
        "                    if k != num:\n",
        "                        str4 += (\",\" + segment_set[k])\n",
        "                str4 += '?'\n",
        "                str4 = postprocess(str4)\n",
        "                # str4 = 'Q.' + str4\n",
        "                s.append(str4)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOywr6nv37-C",
        "colab_type": "text"
      },
      "source": [
        "####identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omo1upBV3-5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def chunk_search(segment, chunked):\n",
        "    m = len(chunked)\n",
        "    list1 = []\n",
        "    for j in range(m):\n",
        "        if (len(chunked[j]) > 2 or len(chunked[j]) == 1):\n",
        "            list1.append(j)\n",
        "        if (len(chunked[j]) == 2):\n",
        "            try:\n",
        "                str1 = chunked[j][0][0] + \" \" + chunked[j][1][0]\n",
        "            except Exception:\n",
        "                pass\n",
        "            else:\n",
        "                if (str1 in segment) == True:\n",
        "                    list1.append(j)\n",
        "    return list1\n",
        "\n",
        "def segment_identify(sen):\n",
        "    segment_set = sen.split(\",\")\n",
        "    return segment_set\n",
        "\n",
        "\n",
        "def clause_identify(segment):\n",
        "    tok = nltk.word_tokenize(segment)\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?|VB.?|MD|RP>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "\n",
        "    flag = 0\n",
        "    for j in range(len(chunked)):\n",
        "        if (len(chunked[j]) > 2):\n",
        "            flag = 1\n",
        "        if (len(chunked[j]) == 2):\n",
        "            try:\n",
        "                str1 = chunked[j][0][0] + \" \" + chunked[j][1][0]\n",
        "            except Exception:\n",
        "                pass\n",
        "            else:\n",
        "                if (str1 in segment) == True:\n",
        "                    flag = 1\n",
        "        if flag == 1:\n",
        "            break\n",
        "\n",
        "    return flag\n",
        "\n",
        "\n",
        "def verbphrase_identify(clause):\n",
        "    tok = nltk.word_tokenize(clause)\n",
        "    tag = nltk.pos_tag(tok)\n",
        "    gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "    chunkparser = nltk.RegexpParser(gram)\n",
        "    chunked = chunkparser.parse(tag)\n",
        "    str1 = \"\"\n",
        "    str2 = \"\"\n",
        "    str3 = \"\"\n",
        "    list1 = chunk_search(clause, chunked)\n",
        "    if len(list1) != 0:\n",
        "        m = list1[len(list1) - 1]\n",
        "        for j in range(len(chunked[m])):\n",
        "            str1 += chunked[m][j][0]\n",
        "            str1 += \" \"\n",
        "\n",
        "    tok1 = nltk.word_tokenize(str1)\n",
        "    tag1 = nltk.pos_tag(tok1)\n",
        "    gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*}\"\"\"\n",
        "    chunkparser1 = nltk.RegexpParser(gram1)\n",
        "    chunked1 = chunkparser1.parse(tag1)\n",
        "\n",
        "    list2 = chunk_search(str1, chunked1)\n",
        "    if len(list2) != 0:\n",
        "\n",
        "        m = list2[0]\n",
        "        for j in range(len(chunked1[m])):\n",
        "            str2 += (chunked1[m][j][0] + \" \")\n",
        "\n",
        "    tok1 = nltk.word_tokenize(str1)\n",
        "    tag1 = nltk.pos_tag(tok1)\n",
        "    gram1 = r\"\"\"chunk:{<VB.?|MD|RP>+}\"\"\"\n",
        "    chunkparser1 = nltk.RegexpParser(gram1)\n",
        "    chunked2 = chunkparser1.parse(tag1)\n",
        "\n",
        "    list3 = chunk_search(str1, chunked2)\n",
        "    if len(list3) != 0:\n",
        "\n",
        "        m = list3[0]\n",
        "        for j in range(len(chunked2[m])):\n",
        "            str3 += (chunked2[m][j][0] + \" \")\n",
        "\n",
        "    X = \"\"\n",
        "    str4 = \"\"\n",
        "    st = nltk.word_tokenize(str3)\n",
        "    if len(st) > 1:\n",
        "        X = st[0]\n",
        "        s = \"\"\n",
        "        for k in range(1, len(st)):\n",
        "            s += st[k]\n",
        "            s += \" \"\n",
        "        str3 = s\n",
        "        str4 = X + \" \" + str2 + str3\n",
        "\n",
        "    if len(st) == 1:\n",
        "        tag1 = nltk.pos_tag(st)\n",
        "        if tag1[0][0] != 'are' and tag1[0][0] != 'were' and tag1[0][0] != 'is' and tag1[0][0] != 'am':\n",
        "            if tag1[0][1] == 'VB' or tag1[0][1] == 'VBP':\n",
        "                X = 'do'\n",
        "            if tag1[0][1] == 'VBD' or tag1[0][1] == 'VBN':\n",
        "                X = 'did'\n",
        "            if tag1[0][1] == 'VBZ':\n",
        "                X = 'does'\n",
        "            str4 = X + \" \" + str2 + str3\n",
        "        if (tag1[0][0] == 'are' or tag1[0][0] == 'were' or tag1[0][0] == 'is' or tag1[0][0] == 'am'):\n",
        "            str4 = tag1[0][0] + \" \" + str2\n",
        "\n",
        "    return str4\n",
        "\n",
        "\n",
        "def subjectphrase_search(segment_set, num):\n",
        "    str2 = \"\"\n",
        "    for j in range(num - 1, 0, -1):\n",
        "        str1 = \"\"\n",
        "        flag = 0\n",
        "        tok = nltk.word_tokenize(segment_set[j])\n",
        "        tag = nltk.pos_tag(tok)\n",
        "        gram = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+<RB.?>*<VB.?|MD|RP>+}\"\"\"\n",
        "        chunkparser = nltk.RegexpParser(gram)\n",
        "        chunked = chunkparser.parse(tag)\n",
        "\n",
        "        list1 = chunk_search(segment_set[j], chunked)\n",
        "        if len(list1) != 0:\n",
        "            m = list1[len(list1) - 1]\n",
        "            for j in range(len(chunked[m])):\n",
        "                str1 += chunked[m][j][0]\n",
        "                str1 += \" \"\n",
        "\n",
        "            tok1 = nltk.word_tokenize(str1)\n",
        "            tag1 = nltk.pos_tag(tok1)\n",
        "            gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+}\"\"\"\n",
        "            chunkparser1 = nltk.RegexpParser(gram1)\n",
        "            chunked1 = chunkparser1.parse(tag1)\n",
        "\n",
        "            list2 = chunk_search(str1, chunked1)\n",
        "            if len(list2) != 0:\n",
        "                m = list2[len(list2) - 1]\n",
        "                for j in range(len(chunked1[m])):\n",
        "                    str2 += (chunked1[m][j][0] + \" \")\n",
        "                flag = 1\n",
        "\n",
        "        if flag == 0:\n",
        "            tok1 = nltk.word_tokenize(segment_set[j])\n",
        "            tag1 = nltk.pos_tag(tok1)\n",
        "            gram1 = r\"\"\"chunk:{<EX>?<DT>?<JJ.?>*<NN.?|PRP|PRP\\$|POS|IN|DT|CC|VBG|VBN>+}\"\"\"\n",
        "            chunkparser1 = nltk.RegexpParser(gram1)\n",
        "            chunked1 = chunkparser1.parse(tag1)\n",
        "\n",
        "            list2 = chunk_search(str1, chunked1)\n",
        "            st = nltk.word_tokenize(segment_set[j])\n",
        "            if len(chunked1[list2[0]]) == len(st):\n",
        "                str2 = segment_set[j]\n",
        "                flag = 1\n",
        "\n",
        "        if flag == 1:\n",
        "            break\n",
        "\n",
        "    return str2\n",
        "\n",
        "\n",
        "def postprocess(string):\n",
        "    tok = nltk.word_tokenize(string)\n",
        "    tag = nltk.pos_tag(tok)\n",
        "\n",
        "    str1 = tok[0].capitalize()\n",
        "    str1 += \" \"\n",
        "    if len(tok) != 0:\n",
        "        for i in range(1, len(tok)):\n",
        "            if tag[i][1] == \"NNP\":\n",
        "                str1 += tok[i].capitalize()\n",
        "                str1 += \" \"\n",
        "            else:\n",
        "                str1 += tok[i].lower()\n",
        "                str1 += \" \"\n",
        "        tok = nltk.word_tokenize(str1)\n",
        "        str1 = \"\"\n",
        "        for i in range(len(tok)):\n",
        "            if tok[i] == \"i\" or tok[i] == \"we\":\n",
        "                str1 += \"you\"\n",
        "                str1 += \" \"\n",
        "            elif tok[i] == \"my\" or tok[i] == \"our\":\n",
        "                str1 += \"your\"\n",
        "                str1 += \" \"\n",
        "            elif tok[i] == \"your\":\n",
        "                str1 += \"my\"\n",
        "                str1 += \" \"\n",
        "            elif tok[i] == \"you\":\n",
        "                if i - 1 >= 0:\n",
        "                    to = nltk.word_tokenize(tok[i - 1])\n",
        "                    ta = nltk.pos_tag(to)\n",
        "                    # print ta\n",
        "                    if ta[0][1] == 'IN':\n",
        "                        str1 += \"me\"\n",
        "                        str1 += \" \"\n",
        "                    else:\n",
        "                        str1 += \"i\"\n",
        "                        str1 += \" \"\n",
        "                else:\n",
        "                    str1 += \"i \"\n",
        "\n",
        "            elif tok[i] == \"am\":\n",
        "                str1 += \"are\"\n",
        "                str1 += \" \"\n",
        "            else:\n",
        "                str1 += tok[i]\n",
        "                str1 += \" \"\n",
        "\n",
        "    return str1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEhp1JUB4qGV",
        "colab_type": "text"
      },
      "source": [
        "#####Question Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqoqn3p4sl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hNvalidation(sentence):\n",
        "    flag = 1\n",
        "\n",
        "    Length = len(sentence)\n",
        "    if (Length > 4):\n",
        "        for i in range(Length):\n",
        "            if (i+4 < Length):\n",
        "                if (sentence[i]==' ' and sentence[i+1]=='h' and sentence[i+2]==' ' and sentence[i+3]=='N' and sentence[i+4]==' '):\n",
        "                    flag = 0\n",
        "\n",
        "\n",
        "    return flag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfVbyPF4xfj",
        "colab_type": "text"
      },
      "source": [
        "#####nlpner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bivqEc4zTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "def nerTagger(nlp, tokenize):\n",
        "    doc = nlp(tokenize)\n",
        "\n",
        "    finalList = []\n",
        "    array = [[]]\n",
        "    for word in doc:\n",
        "        array[0] = 0\n",
        "        for ner in doc.ents:\n",
        "            if (ner.text == word.text):\n",
        "                finalList.append((word.text, ner.label_))\n",
        "                array[0] = 1\n",
        "        if (array[0] == 0):\n",
        "            finalList.append((word.text, 'O'))\n",
        "\n",
        "    return finalList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REbXC96Glz94",
        "colab_type": "text"
      },
      "source": [
        "#Subjective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2il9ywd2p4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2041befb-6df3-4111-dd62-584b75d3d3a2"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNA_1_YO3BCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ce7d84b0-0d3b-487a-a6ae-79855245bfc8"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwwvi6My2e-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e11c4407-80eb-4609-8ad5-781ef1fb70da"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM1T9xBxl3L5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64bcd4fc-ebe4-4589-fe1b-e3fbe2d673f8"
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from textblob import Word\n",
        "import sys\n",
        "\n",
        "\n",
        "def parse(string):\n",
        "    \"\"\"\n",
        "    Parse a paragraph. Devide it into sentences and try to generate quesstions from each sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        txt = TextBlob(string)\n",
        "        # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n",
        "        for sentence in txt.sentences:\n",
        "            genQuestion(sentence)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "def genQuestion(line):\n",
        "    \"\"\"\n",
        "    outputs question from the given text\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    if type(line) is str:     # If the passed variable is of type string.\n",
        "        line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n",
        "\n",
        "    bucket = {}               # Create an empty dictionary\n",
        "\n",
        "\n",
        "    for i,j in enumerate(line.tags):  # line.tags are the parts-of-speach in English\n",
        "        if j[1] not in bucket:\n",
        "            bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n",
        "    \n",
        "    if verbose:               # In verbose more print the key,values of dictionary\n",
        "        print('\\n','-'*20)\n",
        "        print(line ,'\\n')        \n",
        "        print(\"TAGS:\",line.tags, '\\n')  \n",
        "        print(bucket)\n",
        "    \n",
        "    question = ''            # Create an empty string \n",
        "\n",
        "    # These are the english part-of-speach tags used in this demo program.\n",
        "    #.....................................................................\n",
        "    # NNS     Noun, plural\n",
        "    # JJ  Adjective \n",
        "    # NNP     Proper noun, singular \n",
        "    # VBG     Verb, gerund or present participle \n",
        "    # VBN     Verb, past participle \n",
        "    # VBZ     Verb, 3rd person singular present \n",
        "    # VBD     Verb, past tense \n",
        "    # IN      Preposition or subordinating conjunction \n",
        "    # PRP     Personal pronoun \n",
        "    # NN  Noun, singular or mass \n",
        "    #.....................................................................\n",
        "\n",
        "    # Create a list of tag-combination\n",
        "\n",
        "    l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n",
        "    l2 = ['NNP', 'VBG', 'VBZ']\n",
        "    \n",
        "\n",
        "    l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n",
        "    l4 = ['PRP', 'VBG', 'VBZ']\n",
        "    l5 = ['PRP', 'VBG', 'VBD']\n",
        "    l6 = ['NNP', 'VBG', 'VBD']\n",
        "    l7 = ['NN', 'VBG', 'VBZ']\n",
        "\n",
        "    l8 = ['NNP', 'VBZ', 'JJ']\n",
        "    l9 = ['NNP', 'VBZ', 'NN']\n",
        "\n",
        "    l10 = ['NNP', 'VBZ']\n",
        "    l11 = ['PRP', 'VBZ']\n",
        "    l12 = ['NNP', 'NN', 'IN']\n",
        "    l13 = ['NN', 'VBZ']\n",
        "\n",
        "\n",
        "    # With the use of conditional statements the dictionary is compared with the list created above\n",
        "\n",
        "    \n",
        "    if all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n",
        "        if line.words[bucket['PRP']] in ['she','he']:\n",
        "            question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n",
        "        question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n",
        "\n",
        "    # When the tags are generated 's is split to ' and s. To overcome this issue.\n",
        "    if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"’\":\n",
        "        question = question.replace(\" ’ \",\"'s \")\n",
        "\n",
        "    # Print the genetated questions as output.\n",
        "    if question != '':\n",
        "        print('\\n', 'Question: ' + question )\n",
        "   \n",
        "\n",
        "def main():  \n",
        "    \"\"\"\n",
        "    Accepts a text file as an argument and generates questions from it.\n",
        "    \"\"\"\n",
        "    #verbose mode is activated when we give -v as argument.\n",
        "    global verbose \n",
        "    verbose = False\n",
        "\n",
        "    # Set verbose if -v option is given as argument.\n",
        "    if len(sys.argv) >= 3: \n",
        "        if sys.argv[2] == '-v':\n",
        "            print('Verbose Mode Activated\\n')\n",
        "            verbose = True\n",
        "\n",
        "    # Open the file given as argument in read-only mode.\n",
        "    #filehandle = open(sys.argv[1], 'r')\n",
        "    textinput = '''\n",
        "    Last week, Geoffrey Hinton and his team published two papers that introduced a completely new type of neural network based on so-called capsules. In addition to that, the team published an algorithm, called dynamic routing between capsules, that allows to train such a network.\n",
        "Geoffrey Hinton has spent decades thinking about capsules. Source.\n",
        "\n",
        "For everyone in the deep learning community, this is huge news, and for several reasons. First of all, Hinton is one of the founders of deep learning and an inventor of numerous models and algorithms that are widely used today. Secondly, these papers introduce something completely new, and this is very exciting because it will most likely stimulate additional wave of research and very cool applications.\n",
        "\n",
        "In this post, I will explain why this new architecture is so important, as well as intuition behind it. In the following posts I will dive into technical details.\n",
        "\n",
        "However, before talking about capsules, we need to have a look at CNNs, which are the workhorse of today’s deep learning.\n",
        "Architecture of CapsNet from the original paper.\n",
        "2. CNNs Have Important Drawbacks\n",
        "\n",
        "CNNs (convolutional neural networks) are awesome. They are one of the reasons deep learning is so popular today. They can do amazing things that people used to think computers would not be capable of doing for a long, long time. Nonetheless, they have their limits and they have fundamental drawbacks.\n",
        "\n",
        "Let us consider a very simple and non-technical example. Imagine a face. What are the components? We have the face oval, two eyes, a nose and a mouth. For a CNN, a mere presence of these objects can be a very strong indicator to consider that there is a face in the image. Orientational and relative spatial relationships between these components are not very important to a CNN.\n",
        "To a CNN, both pictures are similar, since they both contain similar elements. Source.\n",
        "\n",
        "How do CNNs work? The main component of a CNN is a convolutional layer. Its job is to detect important features in the image pixels. Layers that are deeper (closer to the input) will learn to detect simple features such as edges and color gradients, whereas higher layers will combine simple features into more complex features. Finally, dense layers at the top of the network will combine very high level features and produce classification predictions.\n",
        "\n",
        "An important thing to understand is that higher-level features combine lower-level features as a weighted sum: activations of a preceding layer are multiplied by the following layer neuron’s weights and added, before being passed to activation nonlinearity. Nowhere in this setup there is pose (translational and rotational) relationship between simpler features that make up a higher level feature. CNN approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the “field of view” of higher layer’s neurons, thus allowing them to detect higher order features in a larger region of the input image. Max pooling is a crutch that made convolutional networks work surprisingly well, achieving superhuman performance in many areas. But do not be fooled by its performance: while CNNs work better than any model before them, max pooling nonetheless is losing valuable information.\n",
        "\n",
        "Hinton himself stated that the fact that max pooling is working so well is a big mistake and a disaster:\n",
        "\n",
        "    Hinton: “The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.”\n",
        "\n",
        "Of course, you can do away with max pooling and still get good results with traditional CNNs, but they still do not solve the key problem:\n",
        "\n",
        "    Internal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects.\n",
        "\n",
        "In the example above, a mere presence of 2 eyes, a mouth and a nose in a picture does not mean there is a face, we also need to know how these objects are oriented relative to each other.\n",
        "3. Hardcoding 3D World into a Neural Net: Inverse Graphics Approach\n",
        "\n",
        "Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. Note that the structure of this representation needs to take into account relative positions of objects. That internal representation is stored in computer’s memory as arrays of geometrical objects and matrices that represent relative positions and orientation of these objects. Then, special software takes that representation and converts it into an image on the screen. This is called rendering.\n",
        "Computer graphics takes internal representation of objects and produces an image. Human brain does the opposite. Capsule networks follow a similar approach to the brain. Source.\n",
        "\n",
        "Inspired by this idea, Hinton argues that brains, in fact, do the opposite of rendering. He calls it inverse graphics: from visual information received by eyes, they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain. This is how recognition happens. And the key idea is that representation of objects in the brain does not depend on view angle.\n",
        "\n",
        "So at this point the question is: how do we model these hierarchical relationships inside of a neural network? The answer comes from computer graphics. In 3D graphics, relationships between 3D objects can be represented by a so-called pose, which is in essence translation plus rotation.\n",
        "\n",
        "Hinton argues that in order to correctly do classification and object recognition, it is important to preserve hierarchical pose relationships between object parts. This is the key intuition that will allow you to understand why capsule theory is so important. It incorporates relative relationships between objects and it is represented numerically as a 4D pose matrix.\n",
        "\n",
        "When these relationships are built into internal representation of data, it becomes very easy for a model to understand that the thing that it sees is just another view of something that it has seen before. Consider the image below. You can easily recognize that this is the Statue of Liberty, even though all the images show it from different angles. This is because internal representation of the Statue of Liberty in your brain does not depend on the view angle. You have probably never seen these exact pictures of it, but you still immediately knew what it was.\n",
        "Your brain can easily recognize this is the same object, even though all photos are taken from different angles. CNNs do not have this capability.\n",
        "\n",
        "For a CNN, this task is really hard because it does not have this build-in understanding of 3D space, but for a CapsNet it is much easier because these relationships are explicitly modeled. The paper that uses this approach was able to cut error rate by 45% as compared to the previous state of the art, which is a huge improvement.\n",
        "\n",
        "Another benefit of the capsule approach is that it is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use (Hinton mentions this in his famous talk about what is wrongs with CNNs). In this sense, the capsule theory is much closer to what the human brain does in practice. In order to learn to tell digits apart, the human brain needs to see only a couple of dozens of examples, hundreds at most. CNNs, on the other hand, need tens of thousands of examples to achieve very good performance, which seems like a brute force approach that is clearly inferior to what we do with our brains.\n",
        "4. What Took It so Long?\n",
        "\n",
        "The idea is really simple, there is no way no one has come up with it before! And the truth is, Hinton has been thinking about this for decades. The reason why there were no publications is simply because there was no technical way to make it work before. One of the reasons is that computers were just not powerful enough in the pre-GPU-based era before around 2012. Another reason is that there was no algorithm that allowed to implement and successfully learn a capsule network (in the same fashion the idea of artificial neurons was around since 1940-s, but it was not until mid 1980-s when backpropagation algorithm showed up and allowed to successfully train deep networks).\n",
        "\n",
        "In the same fashion, the idea of capsules itself is not that new and Hinton has mentioned it before, but there was no algorithm up until now to make it work. This algorithm is called “dynamic routing between capsules”. This algorithm allows capsules to communicate with each other and create representations similar to scene graphs in computer graphics.\n",
        "The capsule network is much better than other models at telling that images in top and bottom rows belong to the same classes, only the view angle is different. The latest papers decreased the error rate by a whopping 45%. Source.\n",
        "'''\n",
        "    print('\\n-----------INPUT TEXT-------------\\n')\n",
        "    print(textinput,'\\n')\n",
        "    print('\\n-----------INPUT END---------------\\n')\n",
        "\n",
        "    # Send the content of text file as string to function parse()\n",
        "    parse(textinput)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------INPUT TEXT-------------\n",
            "\n",
            "\n",
            "    Last week, Geoffrey Hinton and his team published two papers that introduced a completely new type of neural network based on so-called capsules. In addition to that, the team published an algorithm, called dynamic routing between capsules, that allows to train such a network.\n",
            "Geoffrey Hinton has spent decades thinking about capsules. Source.\n",
            "\n",
            "For everyone in the deep learning community, this is huge news, and for several reasons. First of all, Hinton is one of the founders of deep learning and an inventor of numerous models and algorithms that are widely used today. Secondly, these papers introduce something completely new, and this is very exciting because it will most likely stimulate additional wave of research and very cool applications.\n",
            "\n",
            "In this post, I will explain why this new architecture is so important, as well as intuition behind it. In the following posts I will dive into technical details.\n",
            "\n",
            "However, before talking about capsules, we need to have a look at CNNs, which are the workhorse of today’s deep learning.\n",
            "Architecture of CapsNet from the original paper.\n",
            "2. CNNs Have Important Drawbacks\n",
            "\n",
            "CNNs (convolutional neural networks) are awesome. They are one of the reasons deep learning is so popular today. They can do amazing things that people used to think computers would not be capable of doing for a long, long time. Nonetheless, they have their limits and they have fundamental drawbacks.\n",
            "\n",
            "Let us consider a very simple and non-technical example. Imagine a face. What are the components? We have the face oval, two eyes, a nose and a mouth. For a CNN, a mere presence of these objects can be a very strong indicator to consider that there is a face in the image. Orientational and relative spatial relationships between these components are not very important to a CNN.\n",
            "To a CNN, both pictures are similar, since they both contain similar elements. Source.\n",
            "\n",
            "How do CNNs work? The main component of a CNN is a convolutional layer. Its job is to detect important features in the image pixels. Layers that are deeper (closer to the input) will learn to detect simple features such as edges and color gradients, whereas higher layers will combine simple features into more complex features. Finally, dense layers at the top of the network will combine very high level features and produce classification predictions.\n",
            "\n",
            "An important thing to understand is that higher-level features combine lower-level features as a weighted sum: activations of a preceding layer are multiplied by the following layer neuron’s weights and added, before being passed to activation nonlinearity. Nowhere in this setup there is pose (translational and rotational) relationship between simpler features that make up a higher level feature. CNN approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the “field of view” of higher layer’s neurons, thus allowing them to detect higher order features in a larger region of the input image. Max pooling is a crutch that made convolutional networks work surprisingly well, achieving superhuman performance in many areas. But do not be fooled by its performance: while CNNs work better than any model before them, max pooling nonetheless is losing valuable information.\n",
            "\n",
            "Hinton himself stated that the fact that max pooling is working so well is a big mistake and a disaster:\n",
            "\n",
            "    Hinton: “The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.”\n",
            "\n",
            "Of course, you can do away with max pooling and still get good results with traditional CNNs, but they still do not solve the key problem:\n",
            "\n",
            "    Internal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects.\n",
            "\n",
            "In the example above, a mere presence of 2 eyes, a mouth and a nose in a picture does not mean there is a face, we also need to know how these objects are oriented relative to each other.\n",
            "3. Hardcoding 3D World into a Neural Net: Inverse Graphics Approach\n",
            "\n",
            "Computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data. Note that the structure of this representation needs to take into account relative positions of objects. That internal representation is stored in computer’s memory as arrays of geometrical objects and matrices that represent relative positions and orientation of these objects. Then, special software takes that representation and converts it into an image on the screen. This is called rendering.\n",
            "Computer graphics takes internal representation of objects and produces an image. Human brain does the opposite. Capsule networks follow a similar approach to the brain. Source.\n",
            "\n",
            "Inspired by this idea, Hinton argues that brains, in fact, do the opposite of rendering. He calls it inverse graphics: from visual information received by eyes, they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain. This is how recognition happens. And the key idea is that representation of objects in the brain does not depend on view angle.\n",
            "\n",
            "So at this point the question is: how do we model these hierarchical relationships inside of a neural network? The answer comes from computer graphics. In 3D graphics, relationships between 3D objects can be represented by a so-called pose, which is in essence translation plus rotation.\n",
            "\n",
            "Hinton argues that in order to correctly do classification and object recognition, it is important to preserve hierarchical pose relationships between object parts. This is the key intuition that will allow you to understand why capsule theory is so important. It incorporates relative relationships between objects and it is represented numerically as a 4D pose matrix.\n",
            "\n",
            "When these relationships are built into internal representation of data, it becomes very easy for a model to understand that the thing that it sees is just another view of something that it has seen before. Consider the image below. You can easily recognize that this is the Statue of Liberty, even though all the images show it from different angles. This is because internal representation of the Statue of Liberty in your brain does not depend on the view angle. You have probably never seen these exact pictures of it, but you still immediately knew what it was.\n",
            "Your brain can easily recognize this is the same object, even though all photos are taken from different angles. CNNs do not have this capability.\n",
            "\n",
            "For a CNN, this task is really hard because it does not have this build-in understanding of 3D space, but for a CapsNet it is much easier because these relationships are explicitly modeled. The paper that uses this approach was able to cut error rate by 45% as compared to the previous state of the art, which is a huge improvement.\n",
            "\n",
            "Another benefit of the capsule approach is that it is capable of learning to achieve state-of-the art performance by only using a fraction of the data that a CNN would use (Hinton mentions this in his famous talk about what is wrongs with CNNs). In this sense, the capsule theory is much closer to what the human brain does in practice. In order to learn to tell digits apart, the human brain needs to see only a couple of dozens of examples, hundreds at most. CNNs, on the other hand, need tens of thousands of examples to achieve very good performance, which seems like a brute force approach that is clearly inferior to what we do with our brains.\n",
            "4. What Took It so Long?\n",
            "\n",
            "The idea is really simple, there is no way no one has come up with it before! And the truth is, Hinton has been thinking about this for decades. The reason why there were no publications is simply because there was no technical way to make it work before. One of the reasons is that computers were just not powerful enough in the pre-GPU-based era before around 2012. Another reason is that there was no algorithm that allowed to implement and successfully learn a capsule network (in the same fashion the idea of artificial neurons was around since 1940-s, but it was not until mid 1980-s when backpropagation algorithm showed up and allowed to successfully train deep networks).\n",
            "\n",
            "In the same fashion, the idea of capsules itself is not that new and Hinton has mentioned it before, but there was no algorithm up until now to make it work. This algorithm is called “dynamic routing between capsules”. This algorithm allows capsules to communicate with each other and create representations similar to scene graphs in computer graphics.\n",
            "The capsule network is much better than other models at telling that images in top and bottom rows belong to the same classes, only the view angle is different. The latest papers decreased the error rate by a whopping 45%. Source.\n",
            " \n",
            "\n",
            "\n",
            "-----------INPUT END---------------\n",
            "\n",
            "\n",
            " Question: What allows addition?\n",
            "\n",
            " Question: What has Geoffrey thinking?\n",
            "\n",
            " Question: What is everyone?\n",
            "\n",
            " Question: What is First?\n",
            "\n",
            " Question: What s CNNs talking?\n",
            "\n",
            " Question: What is CNN?\n",
            "\n",
            " Question: What is CNN?\n",
            "\n",
            " Question: What is job?\n",
            "\n",
            " Question: What is ’ being?\n",
            "\n",
            " Question: What is setup?\n",
            "\n",
            " Question: What is CNN flowing?\n",
            "\n",
            " Question: What is Max achieving?\n",
            "\n",
            " Question: What is CNNs pooling?\n",
            "\n",
            " Question: What is Hinton working?\n",
            "\n",
            " Question: What needs Note?\n",
            "\n",
            " Question: What is representation?\n",
            "\n",
            " Question: What is rendering?\n",
            "\n",
            " Question: What takes Computer?\n",
            "\n",
            " Question: What does brain?\n",
            "\n",
            " Question: What argues Hinton?\n",
            "\n",
            " Question: What is recognition?\n",
            "\n",
            " Question: What is idea?\n",
            "\n",
            " Question: What comes answer?\n",
            "\n",
            " Question: What is pose?\n",
            "\n",
            " Question: What argues Hinton?\n",
            "\n",
            " Question: What is Statue?\n",
            "\n",
            " Question: What is Statue?\n",
            "\n",
            " Question: What is brain?\n",
            "\n",
            " Question: What is CNN?\n",
            "\n",
            " Question: What uses paper?\n",
            "\n",
            " Question: What is CNN learning?\n",
            "\n",
            " Question: What is sense?\n",
            "\n",
            " Question: What seems CNNs?\n",
            "\n",
            " Question: What is Hinton thinking?\n",
            "\n",
            " Question: What is era?\n",
            "\n",
            " Question: What is Hinton?\n",
            "\n",
            " Question: What is ”?\n",
            "\n",
            " Question: What allows algorithm?\n",
            "\n",
            " Question: What is capsule telling?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIOT641X2Akj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fce713d8-20e6-40b4-d38e-809b7664bf10"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: PERSON\n",
            "Leaves: [('Sam', 'NNP')]\n",
            "Word: ('Sam', 'NNP')\n",
            "Word: ('plays', 'VBZ')\n",
            "Word: ('cricket', 'NN')\n",
            "Word: ('at', 'IN')\n",
            "Word: ('5AM', 'CD')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUiEP38Z4KbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c3d4593e-35af-4fcc-b75b-1ec13596c895"
      },
      "source": [
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGOrHjeUyjJ9",
        "colab_type": "text"
      },
      "source": [
        "##Q generation using Tranformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBvvDwtzysbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown -O  t5_que_gen.zip --id 1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
        "!unzip t5_que_gen.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBdITb7uzC9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1ct1ijzI1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "479723d3-ff86-47af-c21e-5d3cdaa926d0"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzzejeqOzLwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QueGenerator():\n",
        "  def __init__(self):\n",
        "    self.que_model = T5ForConditionalGeneration.from_pretrained('./t5_que_gen_model/t5_base_que_gen/')\n",
        "    self.ans_model = T5ForConditionalGeneration.from_pretrained('./t5_ans_gen_model/t5_base_ans_gen/')\n",
        "\n",
        "    self.que_tokenizer = T5Tokenizer.from_pretrained('./t5_que_gen_model/t5_base_tok_que_gen/')\n",
        "    self.ans_tokenizer = T5Tokenizer.from_pretrained('./t5_ans_gen_model/t5_base_tok_ans_gen/')\n",
        "    \n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    self.que_model = self.que_model.to(self.device)\n",
        "    self.ans_model = self.ans_model.to(self.device)\n",
        "  \n",
        "  def generate(self, text):\n",
        "    answers = self._get_answers(text)\n",
        "    questions = self._get_questions(text, answers)\n",
        "    output = [{'answer': ans, 'question': que} for ans, que in zip(answers, questions)]\n",
        "    return output\n",
        "  \n",
        "  def _get_answers(self, text):\n",
        "    # split into sentences\n",
        "    sents = sent_tokenize(text)\n",
        "\n",
        "    examples = []\n",
        "    for i in range(len(sents)):\n",
        "      input_ = \"\"\n",
        "      for j, sent in enumerate(sents):\n",
        "        if i == j:\n",
        "            sent = \"[HL] %s [HL]\" % sent\n",
        "        input_ = \"%s %s\" % (input_, sent)\n",
        "        input_ = input_.strip()\n",
        "      input_ = input_ + \" </s>\"\n",
        "      examples.append(input_)\n",
        "    \n",
        "    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                # do_sample=False,\n",
        "                                # num_beams = 4,\n",
        "                                )\n",
        "    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    answers = [item.split('[SEP]') for item in dec]\n",
        "    answers = chain(*answers)\n",
        "    answers = [ans.strip() for ans in answers if ans != ' ']\n",
        "    return answers\n",
        "  \n",
        "  def _get_questions(self, text, answers):\n",
        "    examples = []\n",
        "    for ans in answers:\n",
        "      input_text = \"%s [SEP] %s </s>\" % (ans, text)\n",
        "      examples.append(input_text)\n",
        "    \n",
        "    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                num_beams = 4)\n",
        "    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    return dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5roFfp2zPFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "que_generator = QueGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhnykjaiFlR3",
        "colab_type": "text"
      },
      "source": [
        "##Extract text from image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlITl04cGDof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecc50430-a78f-4fc9-dc21-f2de2b2a91bf"
      },
      "source": [
        "!pip install pillow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WiRMUF_GHdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7207f262-ee48-4311-fe0e-718f8495b3c4"
      },
      "source": [
        "!pip install PIL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFwEZdiTFqYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "30d1faf2-205f-441a-aee6-97b3a808a3de"
      },
      "source": [
        "!pip install PIL\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image\n",
        "!sudo apt-get install tesseract-ocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from pdf2image) (7.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DMpR2pVGXLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "1414a1d6-4f29-42d2-d611-c01934128763"
      },
      "source": [
        "pip install opencv-python\n",
        "pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PDFInfoNotInstalledError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, poppler_path)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LD_LIBRARY_PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoppler_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LD_LIBRARY_PATH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdfinfo': 'pdfinfo'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6b3a2e82c448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Store all the pages of the PDF in a variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDF_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Counter to store images of each page of PDF to image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mpoppler_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mpage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdfinfo_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserpw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoppler_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# We start by getting the output format, the buffer processing function and if we need pdftocairo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, poppler_path)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         raise PDFInfoNotInstalledError(\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0;34m\"Unable to get page count. Is poppler installed and in PATH?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
          ]
        }
      ]
    }
  ]
}